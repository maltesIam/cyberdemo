# Plan de Construcci√≥n TDD - CyberDemo

> **Documento:** Plan de Construcci√≥n con TDD Estricto
> **Fecha:** 2026-02-14
> **Estado:** üìã PLANIFICACI√ìN
> **Metodolog√≠a:** Test-Driven Development (Red ‚Üí Green ‚Üí Refactor)

---

## Resumen Ejecutivo

### Estado Actual del Proyecto

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     ESTADO ACTUAL: ~75% COMPLETADO                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  75%           ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ  ‚úÖ Completado (75%)           ‚îÇ  üî¥ Pendiente (25%)                   ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ        ‚îÇ
‚îÇ  ‚Ä¢ Infraestructura Docker       ‚îÇ  ‚Ä¢ W8: Grafos Cytoscape              ‚îÇ
‚îÇ  ‚Ä¢ OpenSearch Templates (17)    ‚îÇ  ‚Ä¢ W11: MCP Servers (35 tools)       ‚îÇ
‚îÇ  ‚Ä¢ Generadores (5)              ‚îÇ  ‚Ä¢ W12: Auto-Triggers (30)           ‚îÇ
‚îÇ  ‚Ä¢ APIs REST (19 endpoints)     ‚îÇ  ‚Ä¢ Demo Scenarios E2E (3)            ‚îÇ
‚îÇ  ‚Ä¢ Frontend (9 p√°ginas)         ‚îÇ  ‚Ä¢ SOAR Endpoints                    ‚îÇ
‚îÇ  ‚Ä¢ Policy Engine                ‚îÇ  ‚Ä¢ Graph Endpoints                   ‚îÇ
‚îÇ  ‚Ä¢ Skill SoulInTheBot           ‚îÇ                                      ‚îÇ
‚îÇ  ‚Ä¢ Tests E2E b√°sicos            ‚îÇ                                      ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Trabajo Pendiente por Workstream

| Workstream             | Componentes | Tests   | Impl   | Esfuerzo |
| ---------------------- | ----------- | ------- | ------ | -------- |
| **W8: Grafos**         | 3           | 4       | 11     | 16h      |
| **W11: MCP Backend**   | 8           | 20      | 9      | 20h      |
| **W11: MCP Frontend**  | 6           | 10      | 8      | 16h      |
| **W11: MCP Data**      | 3           | 9       | 4      | 12h      |
| **W12: Auto-Triggers** | 8           | 24      | 35     | 36h      |
| **Demo Scenarios**     | 3           | 27      | 6      | 12h      |
| **APIs Faltantes**     | 2           | 6       | 4      | 8h       |
| **Total**              | **33**      | **100** | **77** | **120h** |

### L√≠nea Temporal

```
Semana 1          Semana 2          Semana 3          Semana 4
‚îÇ                 ‚îÇ                 ‚îÇ                 ‚îÇ
‚îú‚îÄ W11.1 MCP Bck  ‚îú‚îÄ W8 Grafos      ‚îú‚îÄ W12 Triggers   ‚îú‚îÄ Demo Final
‚îú‚îÄ W11.2 MCP Frt  ‚îú‚îÄ W11.3 MCP Data ‚îú‚îÄ W12 Triggers   ‚îú‚îÄ Integraci√≥n
‚îú‚îÄ APIs Faltantes ‚îú‚îÄ Demo Scenarios ‚îú‚îÄ E2E Tests      ‚îú‚îÄ Polish
‚îÇ                 ‚îÇ                 ‚îÇ                 ‚îÇ
‚ñº                 ‚ñº                 ‚ñº                 ‚ñº
20% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 50% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 80% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 100%
```

---

## Metodolog√≠a TDD Estricta

### Ciclo Red-Green-Refactor

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         CICLO TDD OBLIGATORIO                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                        ‚îÇ
‚îÇ     ‚îÇ   RED    ‚îÇ  1. Escribir test que FALLA                           ‚îÇ
‚îÇ     ‚îÇ  (Fail)  ‚îÇ     - Define el comportamiento esperado               ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     - NO escribir c√≥digo de implementaci√≥n a√∫n        ‚îÇ
‚îÇ          ‚îÇ                                                              ‚îÇ
‚îÇ          ‚ñº                                                              ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                        ‚îÇ
‚îÇ     ‚îÇ  GREEN   ‚îÇ  2. Escribir c√≥digo M√çNIMO para pasar                 ‚îÇ
‚îÇ     ‚îÇ  (Pass)  ‚îÇ     - Solo lo necesario para que el test pase         ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     - No optimizar, no generalizar                    ‚îÇ
‚îÇ          ‚îÇ                                                              ‚îÇ
‚îÇ          ‚ñº                                                              ‚îÇ
‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                        ‚îÇ
‚îÇ     ‚îÇ REFACTOR ‚îÇ  3. Mejorar c√≥digo manteniendo tests verdes           ‚îÇ
‚îÇ     ‚îÇ (Clean)  ‚îÇ     - Eliminar duplicaci√≥n                            ‚îÇ
‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     - Mejorar nombres, estructura                     ‚îÇ
‚îÇ          ‚îÇ                                                              ‚îÇ
‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ Siguiente test                                   ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Reglas TDD Inquebrantables

1. **NUNCA** escribir c√≥digo de producci√≥n sin un test que falle primero
2. **NUNCA** escribir m√°s de un test que falle a la vez
3. **NUNCA** escribir m√°s c√≥digo del necesario para pasar el test
4. **SIEMPRE** ejecutar todos los tests despu√©s de cada cambio
5. **SIEMPRE** refactorizar despu√©s de que el test pase

### Estructura de Tests

```python
# Patr√≥n AAA (Arrange-Act-Assert)
def test_nombre_descriptivo_del_comportamiento():
    # Arrange - Preparar datos y dependencias
    sut = SystemUnderTest()
    input_data = create_test_data()

    # Act - Ejecutar la acci√≥n
    result = sut.method_under_test(input_data)

    # Assert - Verificar resultado
    assert result.status == "expected"
    assert result.value == expected_value
```

---

## Sprint 1: APIs Faltantes + MCP Backend (Semana 1)

### 1.1 SOAR Endpoints (D√≠a 1-2)

#### Tests TDD (escribir PRIMERO)

```python
# backend/tests/test_soar.py

import pytest
from httpx import AsyncClient

# TEST 1: Ejecutar playbook de contenci√≥n
@pytest.mark.asyncio
async def test_run_playbook_contain_executes_action(client: AsyncClient):
    """
    GIVEN un dispositivo v√°lido y un playbook de contenci√≥n
    WHEN se ejecuta POST /soar/actions con action=contain
    THEN debe retornar action_id y status=success
    """
    # Arrange
    payload = {
        "action": "contain",
        "device_id": "DEV-001",
        "reason": "Malware detected"
    }

    # Act
    response = await client.post("/soar/actions", json=payload)

    # Assert
    assert response.status_code == 201
    data = response.json()
    assert "action_id" in data
    assert data["status"] == "success"
    assert data["action"] == "contain"


# TEST 2: Ejecutar playbook de kill process
@pytest.mark.asyncio
async def test_run_playbook_kill_process_terminates_process(client: AsyncClient):
    """
    GIVEN un proceso malicioso identificado
    WHEN se ejecuta POST /soar/actions con action=kill_process
    THEN debe terminar el proceso y retornar success
    """
    payload = {
        "action": "kill_process",
        "device_id": "DEV-001",
        "process_id": 12345,
        "reason": "Suspicious behavior"
    }

    response = await client.post("/soar/actions", json=payload)

    assert response.status_code == 201
    data = response.json()
    assert data["status"] == "success"
    assert data["process_terminated"] == True


# TEST 3: Playbook crea log de acci√≥n
@pytest.mark.asyncio
async def test_playbook_creates_action_log(client: AsyncClient):
    """
    GIVEN una acci√≥n ejecutada exitosamente
    WHEN se consulta el log de acciones
    THEN debe existir un registro de la acci√≥n
    """
    # Arrange - ejecutar acci√≥n primero
    payload = {"action": "contain", "device_id": "DEV-002", "reason": "Test"}
    action_response = await client.post("/soar/actions", json=payload)
    action_id = action_response.json()["action_id"]

    # Act
    log_response = await client.get(f"/soar/actions/{action_id}")

    # Assert
    assert log_response.status_code == 200
    log = log_response.json()
    assert log["action_id"] == action_id
    assert "timestamp" in log
    assert log["actor"] == "system"


# TEST 4: Listar acciones por dispositivo
@pytest.mark.asyncio
async def test_list_actions_by_device(client: AsyncClient):
    """
    GIVEN m√∫ltiples acciones en un dispositivo
    WHEN se consulta GET /soar/actions?device_id=X
    THEN debe retornar todas las acciones de ese dispositivo
    """
    response = await client.get("/soar/actions?device_id=DEV-001")

    assert response.status_code == 200
    data = response.json()
    assert "actions" in data
    assert all(a["device_id"] == "DEV-001" for a in data["actions"])


# TEST 5: Playbook con dispositivo inv√°lido
@pytest.mark.asyncio
async def test_playbook_invalid_device_returns_404(client: AsyncClient):
    """
    GIVEN un device_id que no existe
    WHEN se intenta ejecutar una acci√≥n
    THEN debe retornar 404
    """
    payload = {"action": "contain", "device_id": "INVALID", "reason": "Test"}

    response = await client.post("/soar/actions", json=payload)

    assert response.status_code == 404


# TEST 6: Playbook con acci√≥n inv√°lida
@pytest.mark.asyncio
async def test_playbook_invalid_action_returns_400(client: AsyncClient):
    """
    GIVEN una acci√≥n no soportada
    WHEN se intenta ejecutar
    THEN debe retornar 400 Bad Request
    """
    payload = {"action": "invalid_action", "device_id": "DEV-001", "reason": "Test"}

    response = await client.post("/soar/actions", json=payload)

    assert response.status_code == 400
```

#### Implementaci√≥n (escribir DESPU√âS de tests)

```
backend/src/api/soar.py
‚îú‚îÄ‚îÄ POST /soar/actions          ‚Üí run_playbook()
‚îú‚îÄ‚îÄ GET /soar/actions/{id}      ‚Üí get_action()
‚îî‚îÄ‚îÄ GET /soar/actions           ‚Üí list_actions()

backend/src/services/soar_service.py
‚îú‚îÄ‚îÄ execute_contain()
‚îú‚îÄ‚îÄ execute_kill_process()
‚îú‚îÄ‚îÄ create_action_log()
‚îî‚îÄ‚îÄ get_action_log()
```

#### Checklist TDD SOAR

- [ ] üî¥ `test_run_playbook_contain` ‚Üí escribir test
- [ ] üü¢ `test_run_playbook_contain` ‚Üí implementar hasta pasar
- [ ] üîµ Refactorizar
- [ ] üî¥ `test_run_playbook_kill_process` ‚Üí escribir test
- [ ] üü¢ `test_run_playbook_kill_process` ‚Üí implementar hasta pasar
- [ ] üîµ Refactorizar
- [ ] üî¥ `test_playbook_creates_action_log` ‚Üí escribir test
- [ ] üü¢ `test_playbook_creates_action_log` ‚Üí implementar hasta pasar
- [ ] üîµ Refactorizar
- [ ] üî¥ `test_list_actions_by_device` ‚Üí escribir test
- [ ] üü¢ `test_list_actions_by_device` ‚Üí implementar hasta pasar
- [ ] üîµ Refactorizar
- [ ] üî¥ `test_playbook_invalid_device_returns_404` ‚Üí escribir test
- [ ] üü¢ `test_playbook_invalid_device_returns_404` ‚Üí implementar hasta pasar
- [ ] üî¥ `test_playbook_invalid_action_returns_400` ‚Üí escribir test
- [ ] üü¢ `test_playbook_invalid_action_returns_400` ‚Üí implementar hasta pasar

---

### 1.2 Graph Endpoints (D√≠a 2-3)

#### Tests TDD

```python
# backend/tests/test_graph.py

import pytest
from httpx import AsyncClient

# TEST 1: Obtener grafo de incidente
@pytest.mark.asyncio
async def test_get_graph_incident_returns_nodes_and_edges(client: AsyncClient):
    """
    GIVEN un incidente con detecciones y activos asociados
    WHEN se solicita GET /graph/incident/{id}
    THEN debe retornar nodos y edges en formato Cytoscape
    """
    response = await client.get("/graph/incident/INC-ANCHOR-001")

    assert response.status_code == 200
    data = response.json()
    assert "nodes" in data
    assert "edges" in data
    assert len(data["nodes"]) > 0
    assert len(data["edges"]) > 0


# TEST 2: Formato de nodos
@pytest.mark.asyncio
async def test_graph_nodes_have_cytoscape_format(client: AsyncClient):
    """
    GIVEN un grafo de incidente
    WHEN se obtienen los nodos
    THEN cada nodo debe tener id, label, type, y data
    """
    response = await client.get("/graph/incident/INC-ANCHOR-001")
    data = response.json()

    for node in data["nodes"]:
        assert "data" in node
        assert "id" in node["data"]
        assert "label" in node["data"]
        assert "type" in node["data"]
        assert node["data"]["type"] in ["incident", "detection", "asset", "process", "hash"]


# TEST 3: Formato de edges
@pytest.mark.asyncio
async def test_graph_edges_have_cytoscape_format(client: AsyncClient):
    """
    GIVEN un grafo de incidente
    WHEN se obtienen los edges
    THEN cada edge debe tener source, target, y relation
    """
    response = await client.get("/graph/incident/INC-ANCHOR-001")
    data = response.json()

    for edge in data["edges"]:
        assert "data" in edge
        assert "source" in edge["data"]
        assert "target" in edge["data"]
        assert "relation" in edge["data"]


# TEST 4: Colores de nodos seg√∫n estado
@pytest.mark.asyncio
async def test_graph_nodes_have_correct_colors(client: AsyncClient):
    """
    GIVEN un grafo con activos en diferentes estados
    WHEN se obtienen los nodos
    THEN los colores deben reflejar el estado:
         - Green: sin riesgo
         - Yellow: riesgo medio
         - Red: riesgo alto
         - Blue: contenido
    """
    response = await client.get("/graph/incident/INC-ANCHOR-001")
    data = response.json()

    asset_nodes = [n for n in data["nodes"] if n["data"]["type"] == "asset"]

    for node in asset_nodes:
        assert "color" in node["data"]
        assert node["data"]["color"] in ["green", "yellow", "red", "blue"]


# TEST 5: Grafo de incidente inexistente
@pytest.mark.asyncio
async def test_graph_incident_not_found_returns_404(client: AsyncClient):
    """
    GIVEN un incidente que no existe
    WHEN se solicita su grafo
    THEN debe retornar 404
    """
    response = await client.get("/graph/incident/INVALID")

    assert response.status_code == 404


# TEST 6: Grafo de sistema completo
@pytest.mark.asyncio
async def test_get_graph_system_returns_overview(client: AsyncClient):
    """
    GIVEN el sistema con m√∫ltiples incidentes
    WHEN se solicita GET /graph/system
    THEN debe retornar un grafo con fuentes, incidentes y activos
    """
    response = await client.get("/graph/system")

    assert response.status_code == 200
    data = response.json()
    assert "nodes" in data

    types = set(n["data"]["type"] for n in data["nodes"])
    assert "source" in types or len(data["nodes"]) > 0
```

#### Implementaci√≥n

```
backend/src/api/graph.py
‚îú‚îÄ‚îÄ GET /graph/incident/{id}    ‚Üí get_incident_graph()
‚îî‚îÄ‚îÄ GET /graph/system           ‚Üí get_system_graph()

backend/src/services/graph_service.py
‚îú‚îÄ‚îÄ build_incident_graph()
‚îú‚îÄ‚îÄ build_system_graph()
‚îú‚îÄ‚îÄ node_to_cytoscape()
‚îî‚îÄ‚îÄ edge_to_cytoscape()
```

#### Checklist TDD Graph

- [ ] üî¥ `test_get_graph_incident` ‚Üí escribir test
- [ ] üü¢ `test_get_graph_incident` ‚Üí implementar
- [ ] üîµ Refactorizar
- [ ] üî¥ `test_graph_nodes_format` ‚Üí escribir test
- [ ] üü¢ `test_graph_nodes_format` ‚Üí implementar
- [ ] üîµ Refactorizar
- [ ] üî¥ `test_graph_edges_format` ‚Üí escribir test
- [ ] üü¢ `test_graph_edges_format` ‚Üí implementar
- [ ] üîµ Refactorizar
- [ ] üî¥ `test_graph_node_colors` ‚Üí escribir test
- [ ] üü¢ `test_graph_node_colors` ‚Üí implementar
- [ ] üî¥ `test_graph_not_found` ‚Üí escribir test
- [ ] üü¢ `test_graph_not_found` ‚Üí implementar
- [ ] üî¥ `test_get_graph_system` ‚Üí escribir test
- [ ] üü¢ `test_get_graph_system` ‚Üí implementar

---

### 1.3 MCP Backend Server (D√≠a 3-5)

#### Arquitectura MCP

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MCP BACKEND SERVER (Puerto 8001)                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                         ‚îÇ
‚îÇ  FastAPI App                                                            ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ /mcp/sse          ‚Üí Server-Sent Events endpoint                   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ /mcp/messages     ‚Üí JSON-RPC messages                             ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îÇ  MCP Tools (19)                                                         ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ SIEM (5)                                                          ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ siem_list_incidents                                           ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ siem_get_incident                                             ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ siem_get_entities                                             ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ siem_add_comment                                              ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ siem_close_incident                                           ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ EDR (6)                                                           ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ edr_list_detections                                           ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ edr_get_detection                                             ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ edr_get_process_tree                                          ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ edr_hunt_hash                                                 ‚îÇ
‚îÇ  ‚îÇ   ‚îú‚îÄ‚îÄ edr_contain_host                                              ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ edr_lift_containment                                          ‚îÇ
‚îÇ  ‚îÇ                                                                      ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Intel (1): intel_get_indicator                                    ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ CTEM (1): ctem_get_asset_risk                                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Approvals (2): approvals_get, approvals_request                   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ Tickets (2): tickets_create, tickets_list                         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ Reports (2): reports_generate_postmortem, reports_get_postmortem  ‚îÇ
‚îÇ                                                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Tests TDD MCP Server

```python
# backend/tests/test_mcp_server.py

import pytest
from httpx import AsyncClient
import json

# TEST 1: MCP Server inicia
@pytest.mark.asyncio
async def test_mcp_server_starts_and_responds(client: AsyncClient):
    """
    GIVEN el servidor MCP configurado
    WHEN se conecta al endpoint SSE
    THEN debe establecer conexi√≥n y responder
    """
    async with client.stream("GET", "/mcp/sse") as response:
        assert response.status_code == 200
        assert response.headers["content-type"] == "text/event-stream"


# TEST 2: Listar herramientas disponibles
@pytest.mark.asyncio
async def test_mcp_lists_available_tools(client: AsyncClient):
    """
    GIVEN una conexi√≥n MCP establecida
    WHEN se env√≠a tools/list
    THEN debe retornar las 19 herramientas
    """
    message = {
        "jsonrpc": "2.0",
        "id": 1,
        "method": "tools/list"
    }

    response = await client.post("/mcp/messages", json=message)

    assert response.status_code == 200
    data = response.json()
    assert "result" in data
    assert "tools" in data["result"]
    assert len(data["result"]["tools"]) == 19


# TEST 3: Ejecutar tool SIEM list_incidents
@pytest.mark.asyncio
async def test_mcp_siem_list_incidents_tool(client: AsyncClient):
    """
    GIVEN el tool siem_list_incidents
    WHEN se invoca sin par√°metros
    THEN debe retornar lista de incidentes
    """
    message = {
        "jsonrpc": "2.0",
        "id": 2,
        "method": "tools/call",
        "params": {
            "name": "siem_list_incidents",
            "arguments": {}
        }
    }

    response = await client.post("/mcp/messages", json=message)

    assert response.status_code == 200
    data = response.json()
    assert "result" in data
    assert "content" in data["result"]


# TEST 4: Ejecutar tool con filtros
@pytest.mark.asyncio
async def test_mcp_siem_list_incidents_with_filters(client: AsyncClient):
    """
    GIVEN el tool siem_list_incidents
    WHEN se invoca con filtros de severidad
    THEN debe retornar solo incidentes cr√≠ticos
    """
    message = {
        "jsonrpc": "2.0",
        "id": 3,
        "method": "tools/call",
        "params": {
            "name": "siem_list_incidents",
            "arguments": {"severity": "critical"}
        }
    }

    response = await client.post("/mcp/messages", json=message)

    data = response.json()
    content = json.loads(data["result"]["content"][0]["text"])
    assert all(i["severity"] == "critical" for i in content["data"])


# TEST 5: Tool EDR contain_host
@pytest.mark.asyncio
async def test_mcp_edr_contain_host_tool(client: AsyncClient):
    """
    GIVEN un dispositivo v√°lido
    WHEN se invoca edr_contain_host
    THEN debe ejecutar contenci√≥n y retornar resultado
    """
    message = {
        "jsonrpc": "2.0",
        "id": 4,
        "method": "tools/call",
        "params": {
            "name": "edr_contain_host",
            "arguments": {
                "device_id": "DEV-001",
                "reason": "Malware detected"
            }
        }
    }

    response = await client.post("/mcp/messages", json=message)

    data = response.json()
    assert "result" in data
    content = json.loads(data["result"]["content"][0]["text"])
    assert content["status"] == "success"


# TEST 6: Tool con par√°metros inv√°lidos
@pytest.mark.asyncio
async def test_mcp_tool_invalid_params_returns_error(client: AsyncClient):
    """
    GIVEN un tool que requiere par√°metros
    WHEN se invoca sin par√°metros requeridos
    THEN debe retornar error
    """
    message = {
        "jsonrpc": "2.0",
        "id": 5,
        "method": "tools/call",
        "params": {
            "name": "siem_get_incident",
            "arguments": {}  # Falta incident_id
        }
    }

    response = await client.post("/mcp/messages", json=message)

    data = response.json()
    assert "error" in data


# TEST 7: Tool inexistente
@pytest.mark.asyncio
async def test_mcp_unknown_tool_returns_error(client: AsyncClient):
    """
    GIVEN un nombre de tool que no existe
    WHEN se intenta invocar
    THEN debe retornar error
    """
    message = {
        "jsonrpc": "2.0",
        "id": 6,
        "method": "tools/call",
        "params": {
            "name": "unknown_tool",
            "arguments": {}
        }
    }

    response = await client.post("/mcp/messages", json=message)

    data = response.json()
    assert "error" in data
    assert data["error"]["code"] == -32601  # Method not found
```

#### Tests por cada Tool (20 tests adicionales)

```python
# backend/tests/test_mcp_tools.py

# SIEM Tools
- test_mcp_siem_get_incident_tool
- test_mcp_siem_get_entities_tool
- test_mcp_siem_add_comment_tool
- test_mcp_siem_close_incident_tool

# EDR Tools
- test_mcp_edr_list_detections_tool
- test_mcp_edr_get_detection_tool
- test_mcp_edr_get_process_tree_tool
- test_mcp_edr_hunt_hash_tool
- test_mcp_edr_lift_containment_tool

# Intel Tools
- test_mcp_intel_get_indicator_tool

# CTEM Tools
- test_mcp_ctem_get_asset_risk_tool

# Approvals Tools
- test_mcp_approvals_get_tool
- test_mcp_approvals_request_tool

# Tickets Tools
- test_mcp_tickets_create_tool
- test_mcp_tickets_list_tool

# Reports Tools
- test_mcp_reports_generate_postmortem_tool
- test_mcp_reports_get_postmortem_tool
```

#### Implementaci√≥n MCP Backend

```
backend/src/mcp/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ server.py                   # FastMCP server setup
‚îú‚îÄ‚îÄ router.py                   # FastAPI router for /mcp/*
‚îú‚îÄ‚îÄ protocol.py                 # JSON-RPC protocol handling
‚îÇ
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base.py                 # Tool base class
‚îÇ   ‚îú‚îÄ‚îÄ siem.py                 # 5 SIEM tools
‚îÇ   ‚îú‚îÄ‚îÄ edr.py                  # 6 EDR tools
‚îÇ   ‚îú‚îÄ‚îÄ intel.py                # 1 Intel tool
‚îÇ   ‚îú‚îÄ‚îÄ ctem.py                 # 1 CTEM tool
‚îÇ   ‚îú‚îÄ‚îÄ approvals.py            # 2 Approval tools
‚îÇ   ‚îú‚îÄ‚îÄ tickets.py              # 2 Ticket tools
‚îÇ   ‚îî‚îÄ‚îÄ reports.py              # 2 Report tools
‚îÇ
‚îî‚îÄ‚îÄ schemas/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îî‚îÄ‚îÄ tool_schemas.py         # JSON Schema for tools
```

#### Checklist TDD MCP Backend

**Server Base:**

- [ ] üî¥ `test_mcp_server_starts` ‚Üí test
- [ ] üü¢ Implementar server base
- [ ] üî¥ `test_mcp_lists_tools` ‚Üí test
- [ ] üü¢ Implementar tool registry
- [ ] üî¥ `test_mcp_tool_invalid_params` ‚Üí test
- [ ] üü¢ Implementar validaci√≥n
- [ ] üî¥ `test_mcp_unknown_tool` ‚Üí test
- [ ] üü¢ Implementar error handling

**SIEM Tools (5):**

- [ ] üî¥ `test_mcp_siem_list_incidents` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_siem_get_incident` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_siem_get_entities` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_siem_add_comment` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_siem_close_incident` ‚Üí test
- [ ] üü¢ Implementar

**EDR Tools (6):**

- [ ] üî¥ `test_mcp_edr_list_detections` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_edr_get_detection` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_edr_get_process_tree` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_edr_hunt_hash` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_edr_contain_host` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_edr_lift_containment` ‚Üí test
- [ ] üü¢ Implementar

**Otros Tools (6):**

- [ ] üî¥ `test_mcp_intel_get_indicator` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_ctem_get_asset_risk` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_approvals_get` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_approvals_request` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_tickets_create` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_tickets_list` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_reports_generate` ‚Üí test
- [ ] üü¢ Implementar
- [ ] üî¥ `test_mcp_reports_get` ‚Üí test
- [ ] üü¢ Implementar

---

## Sprint 2: W8 Grafos + MCP Frontend/Data (Semana 2)

### 2.1 W8: Frontend Grafos con Cytoscape.js (D√≠a 1-3)

#### Tests TDD (Playwright + React Testing Library)

```typescript
// frontend/tests/graph.spec.ts

import { test, expect } from "@playwright/test";

// TEST 1: Grafo se renderiza
test("graph component renders with nodes", async ({ page }) => {
  // Arrange
  await page.goto("/incidents/INC-ANCHOR-001");

  // Act
  await page.click('[data-testid="view-graph-btn"]');

  // Assert
  const graphContainer = page.locator('[data-testid="cytoscape-graph"]');
  await expect(graphContainer).toBeVisible();

  // Verificar que hay nodos
  const nodes = await page.evaluate(() => {
    // @ts-ignore
    return window.cy.nodes().length;
  });
  expect(nodes).toBeGreaterThan(0);
});

// TEST 2: Nodos son clickeables
test("graph nodes are clickable and open panel", async ({ page }) => {
  await page.goto("/incidents/INC-ANCHOR-001");
  await page.click('[data-testid="view-graph-btn"]');

  // Click en un nodo de asset
  await page.evaluate(() => {
    const assetNode = window.cy.nodes('[type="asset"]').first();
    assetNode.emit("tap");
  });

  // Panel de detalle debe abrirse
  const panel = page.locator('[data-testid="node-detail-panel"]');
  await expect(panel).toBeVisible();
});

// TEST 3: Panel muestra informaci√≥n correcta
test("node panel shows correct sections", async ({ page }) => {
  await page.goto("/incidents/INC-ANCHOR-001");
  await page.click('[data-testid="view-graph-btn"]');

  // Click en nodo
  await page.evaluate(() => {
    window.cy.nodes().first().emit("tap");
  });

  // Verificar 4 secciones
  await expect(page.locator('[data-testid="section-asset-info"]')).toBeVisible();
  await expect(page.locator('[data-testid="section-threat"]')).toBeVisible();
  await expect(page.locator('[data-testid="section-recommendation"]')).toBeVisible();
  await expect(page.locator('[data-testid="section-status"]')).toBeVisible();
});

// TEST 4: Colores de nodos correctos
test("graph nodes have correct colors based on risk", async ({ page }) => {
  await page.goto("/incidents/INC-ANCHOR-001");
  await page.click('[data-testid="view-graph-btn"]');

  const colors = await page.evaluate(() => {
    return window.cy.nodes('[type="asset"]').map((n) => ({
      id: n.id(),
      color: n.style("background-color"),
    }));
  });

  // Verificar que hay colores v√°lidos
  for (const node of colors) {
    expect(
      ["green", "yellow", "red", "blue"].some(
        (c) => node.color.includes(c) || node.color.match(/#[0-9a-f]{6}/i),
      ),
    ).toBeTruthy();
  }
});

// TEST 5: Zoom y pan funcionan
test("graph supports zoom and pan", async ({ page }) => {
  await page.goto("/incidents/INC-ANCHOR-001");
  await page.click('[data-testid="view-graph-btn"]');

  const initialZoom = await page.evaluate(() => window.cy.zoom());

  // Zoom in
  await page.click('[data-testid="zoom-in-btn"]');

  const newZoom = await page.evaluate(() => window.cy.zoom());
  expect(newZoom).toBeGreaterThan(initialZoom);
});

// TEST 6: Layout autom√°tico
test("graph has automatic layout", async ({ page }) => {
  await page.goto("/incidents/INC-ANCHOR-001");
  await page.click('[data-testid="view-graph-btn"]');

  // Ejecutar layout
  await page.click('[data-testid="auto-layout-btn"]');

  // Verificar que los nodos no est√°n todos en la misma posici√≥n
  const positions = await page.evaluate(() => {
    return window.cy.nodes().map((n) => n.position());
  });

  const uniquePositions = new Set(positions.map((p) => `${p.x},${p.y}`));
  expect(uniquePositions.size).toBeGreaterThan(1);
});
```

#### Implementaci√≥n Frontend Grafos

```
frontend/src/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ Graph/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.tsx              # Export principal
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CytoscapeGraph.tsx     # Componente Cytoscape
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ GraphControls.tsx      # Zoom, layout buttons
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ NodeDetailPanel.tsx    # Panel lateral
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ useGraphData.ts        # Hook para datos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ styles.ts              # Estilos Cytoscape
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types.ts               # TypeScript types
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ NodePanel/
‚îÇ       ‚îú‚îÄ‚îÄ AssetInfoSection.tsx   # (a) Qui√©n es el activo
‚îÇ       ‚îú‚îÄ‚îÄ ThreatSection.tsx      # (b) Cu√°l es la amenaza
‚îÇ       ‚îú‚îÄ‚îÄ RecommendationSection.tsx # (c) Qu√© recomienda
‚îÇ       ‚îî‚îÄ‚îÄ StatusSection.tsx      # (d) Estado contenci√≥n/ticket
‚îÇ
‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îî‚îÄ‚îÄ GraphPage.tsx              # P√°gina de grafo
‚îÇ
‚îî‚îÄ‚îÄ services/
    ‚îî‚îÄ‚îÄ graphApi.ts                # API calls para grafos
```

#### Checklist TDD W8 Grafos

- [ ] üî¥ `test_graph_renders` ‚Üí escribir test
- [ ] üü¢ Implementar CytoscapeGraph base
- [ ] üîµ Refactorizar
- [ ] üî¥ `test_nodes_clickable` ‚Üí escribir test
- [ ] üü¢ Implementar click handlers
- [ ] üîµ Refactorizar
- [ ] üî¥ `test_panel_sections` ‚Üí escribir test
- [ ] üü¢ Implementar NodeDetailPanel
- [ ] üîµ Refactorizar
- [ ] üî¥ `test_node_colors` ‚Üí escribir test
- [ ] üü¢ Implementar color mapping
- [ ] üîµ Refactorizar
- [ ] üî¥ `test_zoom_pan` ‚Üí escribir test
- [ ] üü¢ Implementar GraphControls
- [ ] üîµ Refactorizar
- [ ] üî¥ `test_auto_layout` ‚Üí escribir test
- [ ] üü¢ Implementar layout options
- [ ] üîµ Refactorizar

---

### 2.2 MCP Frontend Server (D√≠a 3-4)

#### Tests TDD

```typescript
// frontend/tests/mcp-server.spec.ts

import { test, expect } from "@playwright/test";

// TEST 1: MCP WebSocket se conecta
test("mcp websocket connects successfully", async ({ page }) => {
  await page.goto("/");

  // Verificar conexi√≥n WebSocket
  const wsConnected = await page.evaluate(() => {
    return new Promise((resolve) => {
      const ws = new WebSocket("ws://localhost:3001/mcp");
      ws.onopen = () => resolve(true);
      ws.onerror = () => resolve(false);
    });
  });

  expect(wsConnected).toBe(true);
});

// TEST 2: Tool show_simulation actualiza UI
test("show_simulation tool updates dashboard", async ({ page }) => {
  await page.goto("/dashboard");

  // Simular llamada MCP
  await page.evaluate(() => {
    window.mcpHandler.handleTool("show_simulation", {
      incident_id: "INC-001",
      phase: "investigation",
    });
  });

  // Verificar que la simulaci√≥n se muestra
  const simulation = page.locator('[data-testid="simulation-overlay"]');
  await expect(simulation).toBeVisible();
});

// TEST 3: Tool generate_chart crea gr√°fico
test("generate_chart tool creates chart", async ({ page }) => {
  await page.goto("/dashboard");

  const initialCharts = await page.locator(".chart-container").count();

  await page.evaluate(() => {
    window.mcpHandler.handleTool("generate_chart", {
      type: "bar",
      data: { labels: ["A", "B"], values: [10, 20] },
      title: "Test Chart",
    });
  });

  const newCharts = await page.locator(".chart-container").count();
  expect(newCharts).toBe(initialCharts + 1);
});

// TEST 4: Tool highlight_asset resalta activo
test("highlight_asset tool highlights asset in graph", async ({ page }) => {
  await page.goto("/graph/system");

  await page.evaluate(() => {
    window.mcpHandler.handleTool("highlight_asset", {
      asset_id: "ASSET-001",
    });
  });

  // Verificar que el nodo est√° resaltado
  const isHighlighted = await page.evaluate(() => {
    const node = window.cy.$("#ASSET-001");
    return node.hasClass("highlighted");
  });

  expect(isHighlighted).toBe(true);
});
```

#### Implementaci√≥n MCP Frontend

```
frontend/src/mcp/
‚îú‚îÄ‚îÄ server.ts                   # MCP WebSocket server
‚îú‚îÄ‚îÄ handler.ts                  # Tool handler registry
‚îú‚îÄ‚îÄ types.ts                    # TypeScript types
‚îÇ
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ index.ts
‚îÇ   ‚îú‚îÄ‚îÄ simulation.ts           # show_simulation
‚îÇ   ‚îú‚îÄ‚îÄ charts.ts               # generate_chart
‚îÇ   ‚îú‚îÄ‚îÄ demo.ts                 # run_demo_scenario, get_demo_state
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.ts            # update_dashboard
‚îÇ   ‚îú‚îÄ‚îÄ timeline.ts             # show_alert_timeline
‚îÇ   ‚îú‚îÄ‚îÄ assets.ts               # highlight_asset
‚îÇ   ‚îî‚îÄ‚îÄ postmortem.ts           # show_postmortem
‚îÇ
‚îî‚îÄ‚îÄ context/
    ‚îî‚îÄ‚îÄ MCPContext.tsx          # React context for MCP state
```

---

### 2.3 MCP Data Server (D√≠a 4-5)

#### Tests TDD

```python
# backend/tests/test_mcp_data.py

# TEST 1: Data MCP server inicia
@pytest.mark.asyncio
async def test_data_mcp_server_starts(client: AsyncClient):
    """Data MCP server debe iniciar correctamente"""
    response = await client.get("/data-mcp/health")
    assert response.status_code == 200


# TEST 2: Tool generate_assets
@pytest.mark.asyncio
async def test_data_mcp_generate_assets_tool(client: AsyncClient):
    """generate_assets debe crear activos sint√©ticos"""
    message = {
        "jsonrpc": "2.0",
        "id": 1,
        "method": "tools/call",
        "params": {
            "name": "generate_assets",
            "arguments": {"count": 10, "seed": 42}
        }
    }

    response = await client.post("/data-mcp/messages", json=message)

    data = response.json()
    assert "result" in data
    content = json.loads(data["result"]["content"][0]["text"])
    assert content["generated"] == 10


# TEST 3: Tool generate_all
@pytest.mark.asyncio
async def test_data_mcp_generate_all_tool(client: AsyncClient):
    """generate_all debe generar todos los datos"""
    message = {
        "jsonrpc": "2.0",
        "id": 2,
        "method": "tools/call",
        "params": {
            "name": "generate_all",
            "arguments": {"seed": 42}
        }
    }

    response = await client.post("/data-mcp/messages", json=message)

    data = response.json()
    content = json.loads(data["result"]["content"][0]["text"])
    assert "assets" in content
    assert "detections" in content
    assert "incidents" in content


# TEST 4: Tool reset
@pytest.mark.asyncio
async def test_data_mcp_reset_tool(client: AsyncClient):
    """reset debe limpiar todos los datos"""
    message = {
        "jsonrpc": "2.0",
        "id": 3,
        "method": "tools/call",
        "params": {
            "name": "reset",
            "arguments": {}
        }
    }

    response = await client.post("/data-mcp/messages", json=message)

    data = response.json()
    content = json.loads(data["result"]["content"][0]["text"])
    assert content["status"] == "reset_complete"
```

---

## Sprint 3: W12 Auto-Triggers (Semana 3)

### 3.1 Gateway Client Base (D√≠a 1)

#### Tests TDD

```python
# backend/tests/triggers/test_gateway_client.py

import pytest
from unittest.mock import AsyncMock, patch
from src.triggers.gateway_client import GatewayClient

# TEST 1: Env√≠a mensaje correctamente
@pytest.mark.asyncio
async def test_gateway_client_sends_message():
    """El cliente debe enviar mensaje al gateway"""
    with patch('httpx.AsyncClient.post') as mock_post:
        mock_post.return_value.json.return_value = {"response_id": "resp-001"}
        mock_post.return_value.status_code = 200

        client = GatewayClient("http://localhost:18789")
        result = await client.send_command("/investigate INC-001")

        assert result == "resp-001"
        mock_post.assert_called_once()


# TEST 2: Respeta cooldown
@pytest.mark.asyncio
async def test_gateway_client_respects_cooldown():
    """No debe enviar si est√° en cooldown"""
    client = GatewayClient("http://localhost:18789")

    with patch('httpx.AsyncClient.post') as mock_post:
        mock_post.return_value.json.return_value = {"response_id": "resp-001"}
        mock_post.return_value.status_code = 200

        # Primera llamada
        await client.send_command("/investigate INC-001", cooldown_key="INC-001")

        # Segunda llamada (en cooldown)
        result = await client.send_command("/investigate INC-001", cooldown_key="INC-001")

        assert result is None
        assert mock_post.call_count == 1  # Solo una llamada


# TEST 3: Deduplica mensajes
@pytest.mark.asyncio
async def test_gateway_client_deduplicates():
    """No debe enviar mensajes duplicados"""
    client = GatewayClient("http://localhost:18789")

    with patch('httpx.AsyncClient.post') as mock_post:
        mock_post.return_value.json.return_value = {"response_id": "resp-001"}

        await client.send_command("/investigate INC-001", dedup_key="INC-001")
        await client.send_command("/investigate INC-001", dedup_key="INC-001")

        assert mock_post.call_count == 1


# TEST 4: Maneja errores gracefully
@pytest.mark.asyncio
async def test_gateway_client_handles_errors():
    """Debe manejar errores sin crashear"""
    client = GatewayClient("http://localhost:18789")

    with patch('httpx.AsyncClient.post') as mock_post:
        mock_post.side_effect = Exception("Connection refused")

        result = await client.send_command("/investigate INC-001")

        assert result is None  # No crash, retorna None
```

### 3.2 Trigger Handlers (D√≠a 2-4)

#### Tests TDD por Categor√≠a

```python
# backend/tests/triggers/test_siem_triggers.py

# TEST: incident.created trigger
@pytest.mark.asyncio
async def test_trigger_incident_created_sends_investigate():
    """Nuevo incidente cr√≠tico debe triggerear /investigate"""
    handler = IncidentCreatedHandler(gateway_client)

    incident = Incident(
        incident_id="INC-001",
        severity="critical",
        status="new",
        title="Malware detected"
    )

    with patch.object(gateway_client, 'send_command') as mock:
        await handler.handle(incident)

        mock.assert_called_once_with(
            "/investigate INC-001",
            cooldown_key="INC-001",
            metadata={"trigger": "incident.created", "severity": "critical"}
        )


# TEST: incident.created ignora low severity
@pytest.mark.asyncio
async def test_trigger_incident_created_ignores_low_severity():
    """Incidentes low/medium no deben triggerear"""
    handler = IncidentCreatedHandler(gateway_client)

    incident = Incident(
        incident_id="INC-002",
        severity="low",
        status="new"
    )

    with patch.object(gateway_client, 'send_command') as mock:
        await handler.handle(incident)

        mock.assert_not_called()


# backend/tests/triggers/test_edr_triggers.py

# TEST: detection.propagation trigger
@pytest.mark.asyncio
async def test_trigger_detection_propagation():
    """Hash en m√∫ltiples hosts debe triggerear /hunt"""
    handler = DetectionPropagationHandler(gateway_client)

    hunt_result = HuntResult(
        sha256="abc123",
        total_hosts_found=5,
        hosts=["HOST-1", "HOST-2", "HOST-3", "HOST-4", "HOST-5"]
    )

    with patch.object(gateway_client, 'send_command') as mock:
        await handler.handle(hunt_result)

        mock.assert_called_once()
        call_args = mock.call_args[0][0]
        assert "/hunt abc123" in call_args


# TEST: containment.failed trigger
@pytest.mark.asyncio
async def test_trigger_containment_failed_retries():
    """Fallo de contenci√≥n debe triggerear reintento"""
    handler = ContainmentFailedHandler(gateway_client)

    result = ContainmentResult(
        device_id="DEV-001",
        status="failed",
        reason="Agent unreachable"
    )

    with patch.object(gateway_client, 'send_command') as mock:
        await handler.handle(result)

        mock.assert_called_once()
        assert "/retry-containment DEV-001" in mock.call_args[0][0]


# backend/tests/triggers/test_approval_triggers.py

# TEST: approval.approved trigger
@pytest.mark.asyncio
async def test_trigger_approval_approved_executes():
    """Aprobaci√≥n debe triggerear ejecuci√≥n"""
    handler = ApprovalApprovedHandler(gateway_client)

    approval = ApprovalStatus(
        incident_id="INC-001",
        status="approved",
        decided_by="admin@company.com"
    )

    with patch.object(gateway_client, 'send_command') as mock:
        await handler.handle(approval)

        mock.assert_called_once()
        assert "/execute-containment INC-001" in mock.call_args[0][0]


# TEST: approval.timeout trigger
@pytest.mark.asyncio
async def test_trigger_approval_timeout_escalates():
    """Timeout debe triggerear escalaci√≥n"""
    handler = ApprovalTimeoutHandler(gateway_client)

    with patch.object(gateway_client, 'send_command') as mock:
        await handler.check_timeouts()

        # Si hay timeouts, debe escalar
        if mock.called:
            assert "/escalate-approval" in mock.call_args[0][0]
```

### 3.3 Scheduler e Integraci√≥n (D√≠a 5)

#### Tests TDD

```python
# backend/tests/triggers/test_scheduler.py

@pytest.mark.asyncio
async def test_scheduler_runs_periodic_checks():
    """Scheduler debe ejecutar checks peri√≥dicos"""
    scheduler = TriggerScheduler()

    with patch.object(scheduler, 'check_sla_breaches') as mock:
        await scheduler.start()
        await asyncio.sleep(0.1)

        assert mock.called


# backend/tests/triggers/test_integration.py

@pytest.mark.asyncio
async def test_full_trigger_flow_incident_to_investigate():
    """Flujo completo: nuevo incidente ‚Üí /investigate"""
    # Arrange
    trigger_system = TriggerSystem(gateway_client)

    # Act - Simular nuevo incidente
    incident = create_test_incident(severity="critical")
    await trigger_system.emit("incident.created", incident)

    # Assert - Verificar que se envi√≥ comando
    # (verificar en gateway mock o logs)
```

---

## Sprint 4: Demo Scenarios E2E + Integraci√≥n (Semana 4)

### 4.1 Demo Scenarios E2E (D√≠a 1-2)

#### Tests TDD Escenarios

```python
# tests/e2e/test_scenario_auto_containment.py

@pytest.mark.e2e
async def test_scenario_1_auto_containment_full_flow():
    """
    Escenario 1: Auto-Containment

    GIVEN: Incidente INC-ANCHOR-001 con:
           - Severidad cr√≠tica
           - Intel malicioso
           - Propagaci√≥n detectada
           - Asset NO VIP

    WHEN: Se ejecuta /investigate INC-ANCHOR-001

    THEN:
           - Confidence score >= 90
           - Policy = auto-contain
           - Host contenido
           - Ticket creado
           - Postmortem generado
    """
    # Arrange
    client = CyberDemoApiClient("http://localhost:8000")

    # Verificar precondiciones
    incident = await client.siem.getIncident("INC-ANCHOR-001")
    assert incident.severity == "critical"

    # Act - Ejecutar investigaci√≥n
    result = await investigation_service.investigate("INC-ANCHOR-001")

    # Assert
    assert result.confidence_score >= 90
    assert result.decision.action == "contain"
    assert result.containment_executed == True

    # Verificar ticket
    tickets = await client.tickets.list()
    assert any(t.incident_id == "INC-ANCHOR-001" for t in tickets)

    # Verificar postmortem
    postmortem = await client.reports.getPostmortem("INC-ANCHOR-001")
    assert postmortem is not None


# tests/e2e/test_scenario_vip_approval.py

@pytest.mark.e2e
async def test_scenario_2_vip_requires_approval():
    """
    Escenario 2: VIP Human-in-the-Loop

    GIVEN: Incidente INC-ANCHOR-002 con:
           - Asset VIP
           - Intel malicioso

    WHEN: Se ejecuta /investigate

    THEN:
           - Policy = request_approval
           - Approval card enviado
           - Contenci√≥n espera aprobaci√≥n
    """
    result = await investigation_service.investigate("INC-ANCHOR-002")

    assert result.decision.action == "request_approval"
    assert result.decision.requires_approval == True
    assert result.approval_requested == True
    assert result.containment_executed == False

    # Simular aprobaci√≥n
    await client.approvals.approve("INC-ANCHOR-002", "admin@test.com")

    # Verificar contenci√≥n post-aprobaci√≥n
    result2 = await investigation_service.executeContainmentAfterApproval(
        "INC-ANCHOR-002",
        "DEV-VIP-001",
        ["vip", "executive"]
    )

    assert result2.status == "success"


# tests/e2e/test_scenario_false_positive.py

@pytest.mark.e2e
async def test_scenario_3_false_positive():
    """
    Escenario 3: False Positive

    GIVEN: Incidente INC-ANCHOR-003 con:
           - Intel benign
           - No propagaci√≥n
           - Asset normal

    WHEN: Se ejecuta /investigate

    THEN:
           - Confidence score < 50
           - Policy = mark_false_positive
           - Sin contenci√≥n
           - Incidente cerrado
    """
    result = await investigation_service.investigate("INC-ANCHOR-003")

    assert result.confidence_score < 50
    assert result.decision.action == "mark_false_positive"
    assert result.containment_executed == False

    # Verificar incidente cerrado
    incident = await client.siem.getIncident("INC-ANCHOR-003")
    assert incident.status == "closed" or "false_positive" in incident.tags
```

### 4.2 Integraci√≥n Final (D√≠a 3-4)

#### Tests de Integraci√≥n Completa

```python
# tests/integration/test_full_system.py

@pytest.mark.integration
async def test_mcp_to_investigation_to_ui():
    """
    Test integraci√≥n completa:
    MCP call ‚Üí Investigation ‚Üí UI update
    """
    # 1. Llamada MCP
    mcp_response = await mcp_client.call_tool(
        "siem_get_incident",
        {"incident_id": "INC-ANCHOR-001"}
    )
    assert mcp_response is not None

    # 2. Investigaci√≥n
    result = await investigation_service.investigate("INC-ANCHOR-001")
    assert result.confidence_score > 0

    # 3. Verificar UI (via Playwright)
    # ... verificar que el dashboard muestra el incidente


@pytest.mark.integration
async def test_trigger_to_gateway_to_action():
    """
    Test integraci√≥n:
    Trigger ‚Üí Gateway ‚Üí Claude ‚Üí Action
    """
    # Simular nuevo incidente
    incident = await create_incident(severity="critical")

    # Esperar trigger
    await asyncio.sleep(1)

    # Verificar que se envi√≥ comando al gateway
    # (mock o verificar logs)
```

### 4.3 Polish y Documentaci√≥n (D√≠a 5)

- [ ] Revisar todos los tests pasan
- [ ] Documentar API
- [ ] Actualizar PROGRESS.md al 100%
- [ ] Preparar demo final

---

## Resumen de Tests TDD

### Por Categor√≠a

| Categor√≠a      | Tests Nuevos | Archivos                              |
| -------------- | ------------ | ------------------------------------- |
| SOAR API       | 6            | `test_soar.py`                        |
| Graph API      | 6            | `test_graph.py`                       |
| MCP Backend    | 27           | `test_mcp_*.py`                       |
| MCP Frontend   | 10           | `graph.spec.ts`, `mcp-server.spec.ts` |
| MCP Data       | 9            | `test_mcp_data.py`                    |
| Auto-Triggers  | 24           | `test_*_triggers.py`                  |
| Demo Scenarios | 9            | `test_scenario_*.py`                  |
| Integraci√≥n    | 9            | `test_*_integration.py`               |
| **Total**      | **100**      |                                       |

### Por Sprint

| Sprint    | D√≠as   | Tests   | Implementaci√≥n             |
| --------- | ------ | ------- | -------------------------- |
| Sprint 1  | 5      | 33      | APIs + MCP Backend         |
| Sprint 2  | 5      | 25      | Grafos + MCP Frontend/Data |
| Sprint 3  | 5      | 24      | Auto-Triggers              |
| Sprint 4  | 5      | 18      | Demo + Integraci√≥n         |
| **Total** | **20** | **100** |                            |

---

## Comandos de Ejecuci√≥n

```bash
# Backend tests
cd CyberDemo/backend
pytest tests/ -v --cov=src --cov-report=html

# Frontend tests
cd CyberDemo/frontend
pnpm test

# E2E tests
cd CyberDemo
pnpm test:e2e

# All tests
pnpm test:all
```

---

## Log de Plan

| Fecha      | Cambio                           |
| ---------- | -------------------------------- |
| 2026-02-14 | Plan de construcci√≥n TDD creado  |
| 2026-02-14 | 100 tests definidos              |
| 2026-02-14 | 4 sprints planificados (20 d√≠as) |
